# -*- coding: utf-8 -*-
"""HW2-optimizer-AIstats.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T336KjEpXGHxEoU15emXwO4KNLNJkD24

# DL Optimizers Research 
## AI를 위한 통계학 HW2
1. Nesterov Accelerated Gradient
2. Adadelta
3. AdaMax
4. NAdam
5. AdamW

## 1. Import libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf 
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from keras import backend as K
(x_train, y_train), (x_test, y_test) = mnist.load_data()
print(x_train.shape, y_train.shape)

"""## 2. Load Dataset"""

x_train = x_train.reshape(x_train.shape[0], 28,28,1)
x_test = x_test.reshape(x_test.shape[0],28,28,1)
input_shape=(28,28,1)
y_train = keras.utils.to_categorical(y_train)
y_test = keras.utils.to_categorical(y_test)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

"""## 3. Model Building"""

batch_size=64

num_classes=10

epochs=10

def build_model(optimizer):

    model=Sequential()

    model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=input_shape))

    model.add(MaxPooling2D(pool_size=(2,2)))

    model.add(Dropout(0.25))

    model.add(Flatten())

    model.add(Dense(256, activation='relu'))

    model.add(Dropout(0.5))

    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss=keras.losses.categorical_crossentropy, optimizer= optimizer, metrics=['accuracy'])

    return model

"""## 4. Model Training
1. Nesterov Accelerated Gradient
2. Adadelta
3. AdaMax
4. NAdam
5. AdamW
"""

# define Nesterov Accelerated Gradient 
NAG = tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.0, nesterov=True, name="SGD")

# define AdamW 
AdamW = tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.004, beta_1=0.9, beta_2=0.999,
    epsilon=1e-07, amsgrad=False, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False,
    ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=True, name='AdamW')

# set optimizers 
opitmizers = [NAG,'Adadelta','Adamax','Nadam',AdamW,  # Assignment Optimizers 
              'Adagrad','Adam','RMSprop','SGD']   # General Optimizers

accuracy = {}
loss = {}

for i in opitmizers: 
    model = build_model(i)  
    hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_test,y_test))

    # summarize history for accuracy 
    plt.plot(hist.history['accuracy'])
    plt.plot(hist.history['val_accuracy'])
    if i == AdamW: 
        plt.title('AdamW | model accuracy') 
        accuracy['AdamW'] = hist.history['accuracy']
    elif i == NAG:
        plt.title('Nesterov Accelerated Gradient | model accuracy')       
        accuracy['NAG'] = hist.history['accuracy']
    else:
        plt.title(f'{i} | model accuracy')
        accuracy[i] = hist.history['accuracy']
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

    # summarize history for loss
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    if i == AdamW: 
        plt.title('AdamW | model loss')        
        loss['AdamW'] = hist.history['loss'] 
    elif i == NAG:
        plt.title('Nesterov Accelerated Gradient | model loss')       
        loss['NAG'] = hist.history['loss'] 
    else:
        plt.title(f'{i} | model loss')
        loss[i] = hist.history['loss'] 
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

print(hist.history.keys())

acc_df = pd.DataFrame(accuracy)
acc_df.index.name = 'epochs' 
acc_df

loss_df = pd.DataFrame(loss)
loss_df.index.name = 'epochs'
loss_df

acc_df.to_csv('accuracy-AIstats-optimizers.csv')
loss_df.to_csv('loss-AIstats-optimizers.csv')

